{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unit testing with pytest\n",
    "Steps\n",
    "1) save the .py file starting with test_\n",
    "    ex - test_row_to_list.py\n",
    "2) import pytest and the functions to test\n",
    "3) define a function with name starting with test_\n",
    "4) use assert to test the function with parameters\n",
    "    ex - def test_for_clean_row():\n",
    "            assert row_to_list(\"2,081\\t314,942\\n\") == \\\n",
    "                [\"2,081\",\"314,924\"]\n",
    "        def test_for_missing_area():\n",
    "            assert row_to_list(\"\\t293,410\\n\") is None         \n",
    "5) run the test by !pytest test_row_to_list.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert with log message\n",
    "def test_for_missing_area_with_message.py():\n",
    "    actual = row_to_list(\"\\t293,410\\n\")\n",
    "    expected = None\n",
    "    message = (f\"row_to_list('\\t293,410\\n') \" \"returned {actual} instead \" \"of {expected}\")\n",
    "    \n",
    "    assert actual is expected, message(if the assertion statement is passed nothing is printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acced40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple assertions\n",
    "import pytest\n",
    "...\n",
    "def test_on string_with_one_comma():\n",
    "    return_value = convert_to_int(\"2081\")\n",
    "    assert isinstance(return_value, int)\n",
    "    assert return_value == 2081 # test will pass only if both assertions are passed\n",
    "    \n",
    "#checking rows in training and test sets\n",
    "def test_on_six_rows():\n",
    "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
    "                                 [1148.0, 206186.0], [1506.0, 248419.0],\n",
    "                                 [1210.0, 214114.0], [1697.0, 277794.0]]\n",
    "                                )\n",
    "    # Fill in with training array's expected number of rows\n",
    "    expected_training_array_num_rows = 4\n",
    "    # Fill in with testing array's expected number of rows\n",
    "    expected_testing_array_num_rows = 2\n",
    "    actual = split_into_training_and_testing_sets(example_argument)\n",
    "    # Write the assert statement checking training array's number of rows\n",
    "    assert actual[0].shape[0] == expected_training_array_num_rows, \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)\n",
    "    # Write the assert statement checking testing array's number of rows\n",
    "    assert actual[1].shape[1] == 2, \"The actual number of rows in the testing array is not {}\".format(expected_testing_array_num_rows)\n",
    "    \n",
    "    \n",
    "#raise valueerror\n",
    "with pytest.raises(ValueError):\n",
    "    # <--- Does nothing on entering the context\n",
    "    print('This is part of the context')\n",
    "    # <--- If the context raised ValueError, silence it\n",
    "    # <--- If the context did not raise ValueError, raise an exception\n",
    "    \n",
    "with pytest.raises(ValueError):\n",
    "    raise ValueError # context exits with ValueError\n",
    "    # <--- pytest.raises(ValueError) silences it # no result\n",
    "    \n",
    "with pytest.raises(ValueError):\n",
    "    pass # context exits without raising a ValueError\n",
    "    # <--- pytest.raises(ValueError) raises Failed # prints Failed : Did not raise <class: 'ValueError'>\n",
    "    \n",
    "#ex1\n",
    "def test_valueerror_on_one_dimensional_argument():\n",
    "    example_argument = np.array([2081, 31942, 1059, 106606, 1148, 206186])\n",
    "    with pytest.raises(ValueError):\n",
    "        split_into_training_and_testing_sets(example_arguments)\n",
    "        #if the function raises expected ValueError, test will pass\n",
    "        #if the function is buggy and does not raise ValueError, test will fail\n",
    "        \n",
    "#testing with error message\n",
    "def test_valueerror_on_one_dimensional_argument():\n",
    "    example_argument = np.array([2081, 31942, 1059, 106606, 1148, 206186])\n",
    "    with pytest.raises(ValueError) as exception_info:\n",
    "        split_into_training_and_testing_sets(example_arguments)\n",
    "    #check if ValueError contains correct message\n",
    "    assert exception_info.match(\"Argument data array must be two dimensional. \"\n",
    "                               \"Got 1 dimensional array instead!\")\n",
    "    \n",
    "    #exception_info stores ValueError\n",
    "    #exception_info.match(expected_msg) checks if expected_msg is present in actual error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d8cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytest.approx\n",
    "assert 0.1 + 0.1 + 0.1 == pytest.approx(0.3) # floats have minor variations in python 0.3 can be 0.300000004\n",
    "\n",
    "#use pytest.approx() for numpy array\n",
    "assert np.array([0.1 + 0.1, 0.1 + 0.1]) == pytest.approx(np.array([0.2,0.3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7505bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#types of argument types\n",
    "    #Bad Arguments\n",
    "    #Special Arguments\n",
    "    #Normal Arguments\n",
    "    \n",
    "Table #: 1\n",
    "Character Accuracy: 97.89\n",
    "Layout Accuracy: 91.07\n",
    "Argument        Type    Num rows (training) Num rows (testing)       exceptions\n",
    "One dimensional Bad         -                       -                ValueError\n",
    "Contains 1 row  Bad         -                       -                ValueError\n",
    "Contains 2 rows Special int(0.75 * 2) = 1   2 - int(0.75 * 2) = 1 -\n",
    "Contains 3 rows Special int(0.75 * 3) = 2   3 - int(0.75 * 3) = 1 -\n",
    "Contains 4 rows Special 32\t12\t-\n",
    "Contains 5 rows Special int(0.75 * 5) = 3   5 - int(0.75 * 5) = 2 -\n",
    "Contains 6 rows Normal  int(0.75 * 6) = 4   6 - int(0.75 * 6) = 2 -\n",
    "Contains 8 rows Normal  int(0.75 * 8) = 6   8 - int(0.75 * 6) = 2 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "src/                                           # All application code lives here\n",
    "|-- data/                                      # Package for data preprocessing\n",
    "|   |-- __init__.py\n",
    "|   |- preprocessing_helpers.py                # Contains row_to_list(), convert_to_int()\n",
    "|-- features/                                  # Package for feature generation from preprocessed data\n",
    "|   |-- __init__.py\n",
    "|   |-- as_numpy.py                            # Contains get_data_as_numpy_array()\n",
    "|-- models/                                    # Package for training/testing linear regression model\n",
    "|   |-- __init__.py \n",
    "|   |-- train.py                               # Contains split_into_training_and_testing_sets()\n",
    "\n",
    "tests/                                         # Test suite: all tests live here\n",
    "|-- data/\n",
    "|   |-- __init__.py\n",
    "|   |-- test_preprocessing_helpers.py          # Contains TestRowToList, TestConvertToInt\n",
    "|-- features/\n",
    "|   |--__init__.py\n",
    "|   |-- test_as_numpy.py                       # Contains TestGetDataAsNumpyArray\n",
    "|-- models/\n",
    "|   |--_init__.py\n",
    "|   |-- test_train.py                          # Contains TestSplit IntoTrainingAndTestingSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81289e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex of test_preprocessing_helpers.py\n",
    "import pytest\n",
    "from data.preprocessing_helpers import row_to_list, convert_to_int\n",
    "\n",
    "class TestRowToList(object):\n",
    "    def test_on_no_tab_no_missing_value(self):      # Always put the argument object # Always put the argument self\n",
    "    ...\n",
    "    def test_on_two_tabs_no_missing_value(self):    # Always put the argument self\n",
    "    ...\n",
    "    \n",
    "class TestConvertToInt(object):                     # Test class for convert_to_int()\n",
    "    def test_with_no_comma (self):                  # A test for convert_to_int()\n",
    "    ...\n",
    "    def test_with_one_comma (self):                 # Another test for convert_to_int()\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3640728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running all tests\n",
    "cd tests\n",
    "pytest\n",
    "\n",
    "• Recurses into directory subtree of tests/.\n",
    " • Filenames starting with test_ → test module.\n",
    "  ■ Classnames starting with Test → test class.\n",
    "  ■ Function names starting with test_ → unit test.\n",
    "    \n",
    "# -x flag: stop after first failure\n",
    "pytest -x \n",
    "\n",
    "#running tests in test module\n",
    "pytest data/test_preprocessing_helpers.py\n",
    "\n",
    "#node id\n",
    "• Node ID of a test class: <path to test module> :: <test class name>\n",
    "• Node ID of an unit test: <path to test module> :: <test class name>::<unit test name>\n",
    "                \n",
    "#running test by node id of class TestRowToList()(runs all functions in the class)\n",
    "pytest data/test_preprocessing_helpers.py::TestRowToList\n",
    "        \n",
    "#running test by node id of function test_on_one_tab_with_missing_value()(one runs a single test)\n",
    "pytest data/test_preprocessing_helpers.py::TestRowToList::test_on_one_tab_with_missing_value\n",
    "                \n",
    "# -k running tests with keyword expressions\n",
    "pytest -k \"pattern\"\n",
    "\n",
    "#ex1 (auto identifies and runs all files with TestSplit)\n",
    "pytest -k \"TestSplitIntoTrainingAndTestingSets\" ==  pytest -k \"TestSplit\"\n",
    "#ex2 (auto identifies and runs all files with TestSplit without test_on_one_row)\n",
    "pytest -k \"TestSplit and not test_on_one_row\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7bb660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#expecting the functions to fail\n",
    "class TestTrainModel(object):\n",
    "    @pytest.mark.xfail(reason = \"Using TDD, train_model is not implemented\")\n",
    "    def test_on_linear_data(self):\n",
    "        ...\n",
    "        \n",
    "#conditional skipping\n",
    "class TestConvertToInt(object):\n",
    "    @pytest.mark.skipif(sys.version_info > (2, 7), reason=\"requires Python 2.7\")\n",
    "    def test_with_no_comma (self):\n",
    "        \"\"\"Only runs on Python 2.7 or lower\"\"\"\n",
    "        test_argument = \"756\"\n",
    "        expected = 756\n",
    "        actual = convert_to_int(test_argument)\n",
    "        message = unicode (\"Expected: 2081, Actual: {0}\".format(actual))\n",
    "        assert actual == expected, message\n",
    "        \n",
    "#show reason for skipping\n",
    "pytest -rs\n",
    "\n",
    "#show reason for xfail\n",
    "pytest -rx\n",
    "\n",
    "#show reason for both skipped and xfail\n",
    "pytest -rsx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47709e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#continious integration and code coverage\n",
    "repository root\n",
    "|-- src\n",
    "|-- tests\n",
    "|--.travis.yml\n",
    "\n",
    "#travis.yml\n",
    "Language: python \n",
    "python:\n",
    "    - \"3.6\" \n",
    "install:\n",
    "    - pip install -e\n",
    "    - pip install pytest-cov codecov       # Install packages for code coverage report\n",
    "script:\n",
    "    - pytest --cov=src tests               # Point to the source directory\n",
    "after_success:\n",
    "    - codecov                              # uploads report to codecov.io\n",
    "    \n",
    "#push the file to github\n",
    "git add .travis.yml\n",
    "git push origin master\n",
    "\n",
    "#TravisCI\n",
    "install Travis CI on github account > market place\n",
    "install Codecov on github account > market place\n",
    "\n",
    "#Adding Travis CI badge\n",
    "-reroute to TravisCI dashboard\n",
    "-click build passing > select Markdown> copy Result text > paste text Readme.md >build passing icon will appear\n",
    "\n",
    "#Adding Codecov badge\n",
    "after Tavis CI completes the build >copy the Mardown text from setting > paste text Readme.md >codecov icon will appear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf533ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beyond assertion: setup and teardown\n",
    "#Setup > Assert > Teardown (Workflow)\n",
    "#Setup\n",
    "import os\n",
    "import pytest\n",
    "\n",
    "def test_on_raw_data(raw_and_clean_data_file):\n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    preprocess(raw_path, clean_path)\n",
    "    with open(clean_data_file_path) as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1801\\t201411\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2002\\t333209\\n\"\n",
    "\n",
    "#Fixture\n",
    "@pytest.fixture\n",
    "def raw_and_clean_data_file():\n",
    "    raw_data_file_path = \"raw.txt\"\n",
    "    clean_data_file_path = \"clean.txt\"\n",
    "    with open(raw_data_file_path,\"w\") as f:\n",
    "        f.write(\"1,801\\t201,411\\n\"\n",
    "        \"1,767565,112\\n\"\n",
    "        \"2,002\\t333,209\\n\"\n",
    "        \"1990\\t782,911\\n\"\n",
    "        \"1,285\\t389129\\n\"\n",
    "        )\n",
    "    yield raw_data_file_path, clean_data_file_path # Use yield instead of return\n",
    "    os.remove(raw_data_file_path)        #teardown\n",
    "    os.remove(clean_data_file_path)      #teardown\n",
    "    \n",
    "#Fixture with tmpdir (tmpdir is a fixture function)\n",
    "@pytest.fixture\n",
    "def raw_and_clean_data_file(tmpdir):\n",
    "    raw_data_file_path = tmpdir.join(\"raw.txt\")\n",
    "    clean_data_file_path = tmpdir.join(\"Clean.txt\")\n",
    "    with open(raw_data_file_path,\"w\") as f:\n",
    "        f.write(\"1,801\\t201,411\\n\"\n",
    "        \"1,767565,112\\n\"\n",
    "        \"2,002\\t333,209\\n\"\n",
    "        \"1990\\t782,911\\n\"\n",
    "        \"1,285\\t389129\\n\"\n",
    "        )\n",
    "    yield raw_data_file_path, clean_data_file_path # Use yield instead of return\n",
    "    #No teardown code required\n",
    "    \n",
    "#mocking\n",
    "pip install pytest-mock\n",
    "unittest.mock : python standard library package\n",
    "\n",
    "def row_to_list_bug_free(row):\n",
    "    return_values = {\n",
    "    \"1,801\\t201,411\\n\": [\"1,801\", \"201,411\"],\n",
    "    \"1,767565,112\\n\": None,\n",
    "    \"2,002\\t333, 209\\n\": [\"2,002\", \"333,209\"], \"1990\\t782,911\\n\": [\"1990\", \"782,911\"], \"1,285\\t389129\\n\": [\"1,285\", \"389129\"],\n",
    "    }\n",
    "    return return_values[row]\n",
    "\n",
    "def test_on_raw_data(raw_and_clean_data_file,\n",
    "                     mocker\n",
    "                    ):\n",
    "    raw_path,clean_path = raw_and_clean_data_file\n",
    "    row_to_list_mock = mocker.patch(\n",
    "                        \"data.preprocessing_helpers.row_to_list\",side_effect = row_to_list_bug_free)# returns utittest.mock.MagicMock()\n",
    "    preprocess(raw_path, clean_path)\n",
    "    \n",
    "    assert row_to_list.call_args_list == [\n",
    "        call(\"1,801\\t201,411\\n\"),\n",
    "        call(\"1,767565,112\\n\"),\n",
    "        call(\"2,002\\t333,209\\n\"), call(\"1990\\t782,911\\n\"), call(\"1,285\\t389129\\n\")\n",
    "        ]\n",
    "#checking the arguments\n",
    "from unittest.mock import call\n",
    "\n",
    "#call_args_list\n",
    "row_to_list_mock.call_args_list\n",
    "\n",
    "#Testing Models\n",
    "data\n",
    "|-- raw\n",
    "|    |-- housing_data.txt\n",
    "|-- clean\n",
    "|    |-- clean_housing_data.txt\n",
    "src\n",
    "tests\n",
    "\n",
    "from data.preprocessing_helpers import preprocess\n",
    "from features.as_numpy import get_data_as_numpy_array\n",
    "from models.train import (split_into_training_and_testing_sets)\n",
    "\n",
    "preprocess(\"data/raw/housing_data.txt\",\"data/clean/clean_housing_data.txt\")\n",
    "\n",
    "data = get_data_as_numpy_array(\"data/clean/clean_housing_data.txt\",2)\n",
    "\n",
    "training_set, testing_set = split_into_training_and_testing_sets(data)\n",
    "\n",
    "#linear regression model\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def train_model(training_set):\n",
    "    slope, intercept, _, _, _ = linregress(training_set[:,0], testing_set[:,1])\n",
    "    return slope, intercept\n",
    "\n",
    "#Use dataset where return is known\n",
    "import pytest\n",
    "import numpy as np\n",
    "from models.train import train_model\n",
    "\n",
    "def test_on_linear_data():\n",
    "    test_argument = np.array([[1.0,3.0],\n",
    "                              [2.0,5.0],\n",
    "                              [3.0,7.0]])\n",
    "    expected_slope = 2.0\n",
    "    expected_intercept = 1.0\n",
    "    \n",
    "    slope,intercept = train_model(test_argument)\n",
    "    assert slope == pytest.approx(expected_slope) or assert slope > 0 \n",
    "    assert intercept == pytest.approx(expected_intercept)\n",
    "    \n",
    "#Use Inequalities\n",
    "import numpy as np\n",
    "from models.train import train_model\n",
    "\n",
    "def test_on_positively_correlated_data():\n",
    "    test_argument = np.array([[1.0,4.0],[2.0,4.0],[3.0,9.0],[4.0,10.0],[5.0,7.0],[6.0,13.0]])\n",
    "    slope,intercept = train_model(test_argument)\n",
    "\n",
    "def model_test(testing_set, slope, intercept)\n",
    "    \"\"\"Return r^2 of fit\"\"\"\n",
    "\n",
    "    \n",
    "#Testing plots\n",
    "data/ \n",
    "src/ \n",
    "|-- data/\n",
    "|--features/\n",
    "|--models/\n",
    "|-- visualization\n",
    "|   |-- __init__.py\n",
    "|   |-- test_plots.py  #test_module\n",
    "    |-- baseline       #Contains baselines\n",
    "        |-- test_plot_for_linear_data.png\n",
    "        \n",
    "\n",
    "tests/\n",
    "\n",
    "def best_plot_for_best_fit_line(slope,\n",
    "                               intercept,\n",
    "                               x_array,\n",
    "                               y_array,\n",
    "                               title):\n",
    "    \"\"\"\n",
    "    slope: slope of best line \n",
    "    ;intercept: intercept of best fit line\n",
    "    x_array: array containing housing areas\n",
    "    y_array: array containing housing prices\n",
    "    title: title of the plot\n",
    "    \n",
    "    Returns: matplotlib.figure.Figure()\n",
    "    \"\"\"\n",
    "    \n",
    "#Traininng plot\n",
    "from visualization import get_plot_for_best_fit_line\n",
    "\n",
    "preprocess(...)\n",
    "data = get_data_as_numpy_array(...)\n",
    "training_set, testing_set = (split_into_training_and_testing_sets(data))\n",
    "\n",
    "slope,intercept = train_model(training_set)\n",
    "get_plot_for_best_fit_line(slope,intercept,\n",
    "                          training_set[:,0],training_set[:, 1],\n",
    "                          \"Training\")\n",
    "\n",
    "get_plot_for_best_fit_line(slope, intercept,\n",
    "                          testing_set[:,0], testing_set[:,1],\n",
    "                          \"Testing\")\n",
    "\n",
    "#generate baseline images\n",
    "pytest-mpl\n",
    "\n",
    "#example testing plot\n",
    "import pytest\n",
    "import numpy as np\n",
    "from visuallization import get_plot_for_best_fit_line\n",
    "\n",
    "@pytest.mark.mpl_image_compare # Under the hood baseline generation and comparision\n",
    "def test_plot_for_linear_data():\n",
    "    slope = 2.0\n",
    "    intercept = 1.0\n",
    "    x_array = np.array([1.0, 2.0, 3.0]) #Linear data set\n",
    "    y_array = np.array([3.0,5.0,7.0])\n",
    "    title = \"Test plot for linear data\"\n",
    "    return get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title)\n",
    "\n",
    "#create first png\n",
    "!pytest -k \"test_plot_for_linear_data\" --mpl-generate-path visualization/baseline\n",
    "\n",
    "#run the test\n",
    "!pytest -k \"test_plot_for_linear_data\" --mpl\n",
    "\n",
    "pytest --mpl-generate-path /home/repl/workspace/project/tests/visualization/baseline -k \"test_plot_for_almost_linear_data\"\n",
    "\n",
    "• Testing saves time and effort.\n",
    "• pytest\n",
    "    •Testing return values and exceptions.\n",
    "    •Running tests and reading the test result report.\n",
    "• Best practices\n",
    "    • Well tested function using normal, special and bad arguments.\n",
    "    。 TDD, where tests get written before implementation.\n",
    "    • Test organization and management.\n",
    "• Advanced skills\n",
    "    • Setup and teardown with fixtures, mocking.\n",
    "    • Sanity tests for data science models.\n",
    "    。 Plot testing.\n",
    "    \n",
    "# https://github.com//gutfeeling/univariate-linear-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215efd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com//gutfeeling/univariate-linear-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ebe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
